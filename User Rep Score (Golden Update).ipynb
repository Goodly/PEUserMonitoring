{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Rep Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array of questions that are \"parent\" questions\n",
    "language_parents = [\"T1.Q1\", \"T1.Q12\"]\n",
    "probability_parents = [\"T1.Q1\", \"T1.Q5\", \"T1.Q6\", \"T1.Q11\"]\n",
    "reasoning_parents = [\"T1.Q1\"]\n",
    "evidence_parents = [\"T1.Q1\", \"T1.Q12\"]\n",
    "\n",
    "parents = {\"Evidence\":evidence_parents, \n",
    "           \"Language\":language_parents, \n",
    "           \"Probability\":probability_parents, \n",
    "           \"Reasoning\":reasoning_parents}\n",
    "\n",
    "# corresponding list of question types, language_parents[n] maps to language_parent_types[n]\n",
    "language_parents_types = [\"select_all\", \"ordinal\"]\n",
    "probability_parents_types = [\"ordinal\", \"ordinal\", \"select_one\", \"ordinal\"]\n",
    "reasoning_parents_types = [\"select_all\"]\n",
    "evidence_parents_types = [\"select_one\", \"ordinal\"]\n",
    "\n",
    "# corresponding list of max number of answers, used for scoring ordinal questions\n",
    "language_num_answers = [13, 4]\n",
    "probability_num_answers = [3, 3, 3, 4]\n",
    "reasoning_num_answers = [6]\n",
    "evidence_num_answers = [3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_feed(feed, feed_type):\n",
    "    '''\n",
    "    Format feed into a more processable format:\n",
    "        Row: review id\n",
    "        Column: question number\n",
    "        Value: the review's choice for the corresponding question\n",
    "    \n",
    "    Parameter:\n",
    "        feed: a data csv read by pandas\n",
    "        feed_type: type of the data csv (evidence, for exampel)\n",
    "    '''\n",
    "    tbl = pd.pivot_table(feed, \n",
    "                         values='gold_standard_label', \n",
    "                         index='article_number', \n",
    "                         columns='question_label', \n",
    "                         aggfunc=set)\n",
    "    for col in tbl.columns:\n",
    "        tbl[col].loc[tbl[col].isnull()] = tbl[col].loc[tbl[col].isnull()].apply(lambda x: set({}))\n",
    "        tbl[col] = tbl[col].apply(lambda x: x - set([np.nan]))\n",
    "    return tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reviews of the golden standard article\n",
    "#Should implement an iteration over multiple csvs in the future\n",
    "data = pd.read_csv(\"Gold Standard Column/Source Data for Reference/Language1020.csv\")\n",
    "data_type = 'Language'\n",
    "\n",
    "#Reviewers' current reputation score\n",
    "old_rep_scores = pd.read_csv(\"./User Rep Score/score.csv\").set_index('contributor_uuid')\n",
    "old_rep_scores = {user: old_rep_scores.loc[user, \"score\"] if user in old_rep_scores.index else 0.5 \n",
    "              for user in data[\"contributor_uuid\"].unique()}\n",
    "rep_scores = {user:[] for user in old_rep_scores.keys()}\n",
    "\n",
    "#Golden standard answers, experts' reviews of the golden standard article\n",
    "gold_answers = {\n",
    "    \"Evidence\":format_feed(pd.read_csv(\"Gold Standard Column/Data with Gold Column/evidence_with_gold.csv\"),\"Evidence\"), \n",
    "    \"Language\":format_feed(pd.read_csv(\"Gold Standard Column/Data with Gold Column/language_with_gold.csv\"), \"Language\"), \n",
    "    \"Probability\":format_feed(pd.read_csv(\"Gold Standard Column/Data with Gold Column/probability_with_gold.csv\"), \"Probability\"), \n",
    "    \"Reasoning\":format_feed(pd.read_csv(\"Gold Standard Column/Data with Gold Column/reasoning_with_gold.csv\"), \"Reasoning\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update User Rep Score With Golden Standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_by_articles = {article_id:data[data[\"article_number\"] == article_id] for article_id in data[\"article_number\"]}\n",
    "for article_id in sliced_by_articles.keys():\n",
    "    df = sliced_by_articles[article_id]\n",
    "    df = pd.pivot_table(df,\n",
    "                        values='answer_label', \n",
    "                        index='contributor_uuid', \n",
    "                        columns='question_label', \n",
    "                        aggfunc=set)\n",
    "    gold_answers_of_article = gold_answers[\"Language\"].loc[gold_answers[\"Language\"].index == article_id, :]\n",
    "    for user in df.index:\n",
    "        review = df.loc[df.index == user, :]\n",
    "        n_gold_answers = 0\n",
    "        n_correct = 0\n",
    "        for question_label in df.columns:\n",
    "            if question_label in gold_answers_of_article.columns and len(gold_answers_of_article[question_label].iloc[0]) > 0:\n",
    "                n_gold_answers += 1\n",
    "                if len(review.loc[:, question_label].iloc[0].intersection(gold_answers_of_article[question_label].iloc[0])) > 0:\n",
    "                    n_correct += 1\n",
    "        rep_scores[user].append(n_correct/n_gold_answers)\n",
    "                \n",
    "for user in rep_scores.keys():\n",
    "    rep_scores[user] = 0.5 * sum(rep_scores[user])/len(rep_scores[user]) + 0.5 * old_rep_scores[user]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update the repscore csv file\n",
    "csv = pd.read_csv(\"./User Rep Score/score.csv\").set_index('contributor_uuid')\n",
    "for user in rep_scores.keys():\n",
    "    if user in csv.index:\n",
    "        csv.loc[user, 'score'] = rep_scores[user]\n",
    "    else:\n",
    "        helper_dict = {user:rep_scores[user]}\n",
    "        helper_df = pd.DataFrame.from_dict(helper_dict, orient=\"index\", columns=['score'])\n",
    "        csv = csv.append(helper_df)\n",
    "csv = csv.reset_index()\n",
    "csv.columns = ['contributor_uuid', 'score']\n",
    "csv.to_csv(\"./User Rep Score/score.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging/Helper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(\n",
    "            { 'de68bbf8-46d7-45f0-9111-92e64ab9499a':0.5, #Should be from a csv file, currently using fake data\n",
    "               'f9143626-bfe0-4e69-b652-6d1525ab4eb0':0.5,\n",
    "               '85579cf2-e01c-45c5-b9e7-34b40467148d':0.5,\n",
    "               '00f548b7-6b63-4b47-828e-8e416b6ca0e2':0.5,\n",
    "               '95dc40d9-3710-47d7-abf0-24b825a1d0c5':0.5,\n",
    "               'bd786026-bad5-4fa8-9a3a-38ca03a16412':0.5,\n",
    "               'e1ae8875-a398-4dde-8f4e-4b21109784e3':0.5},\n",
    "    orient = \"index\",\n",
    ").reset_index()\n",
    "df.columns = ['contributor_uuid', 'score']\n",
    "df\n",
    "df.to_csv(\"./User Rep Score/score.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
